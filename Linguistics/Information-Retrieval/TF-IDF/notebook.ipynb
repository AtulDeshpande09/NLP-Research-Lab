{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd3224f-caa4-4c72-b6d8-089cd3401d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bow import BagofWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b707030-78b4-4d4c-b650-b186335cefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"Machine learning is an exciting field that enables computers to learn from data and make intelligent decisions\" \n",
    "t2 = \"Deep learning, a subset of machine learning, uses neural networks to process large amounts of data and improve accuracy\"\n",
    "t3 = \"Artificial intelligence covers various techniques, including machine learning and deep learning, to mimic human intelligence and solve complex problems\"\n",
    "texts = [t1,t2,t3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb20489-48eb-4481-a3b5-d9cddb340f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BagofWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4053cc4c-71cd-4284-8c43-fa9dd400f56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': ['machine',\n",
       "  'learning',\n",
       "  'is',\n",
       "  'an',\n",
       "  'exciting',\n",
       "  'field',\n",
       "  'that',\n",
       "  'enables',\n",
       "  'computers',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'from',\n",
       "  'data',\n",
       "  'and',\n",
       "  'make',\n",
       "  'intelligent',\n",
       "  'decisions',\n",
       "  'deep',\n",
       "  'learning,',\n",
       "  'a',\n",
       "  'subset',\n",
       "  'of',\n",
       "  'uses',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'process',\n",
       "  'large',\n",
       "  'amounts',\n",
       "  'improve',\n",
       "  'accuracy',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'covers',\n",
       "  'various',\n",
       "  'techniques,',\n",
       "  'including',\n",
       "  'mimic',\n",
       "  'human',\n",
       "  'solve',\n",
       "  'complex',\n",
       "  'problems'],\n",
       " 'doc0': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'doc1': [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'doc2': [1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6733f0-2daa-4a47-afe2-c4dd8bdfc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = bag.bow(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b58032b-a61a-4572-9e68-a7ced498533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfidf import TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ad46b9-11bf-4d17-9a3c-1c00e61b7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Created!!!\n"
     ]
    }
   ],
   "source": [
    "ti = TFIDF()\n",
    "\n",
    "ti.get_words(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2127f71-8858-4344-bad3-c1a0d715a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word and doc :  machine doc0\n",
      "word and doc :  machine doc1\n",
      "word and doc :  machine doc2\n",
      "word and doc :  learning doc0\n",
      "word and doc :  learning doc1\n",
      "word and doc :  learning doc2\n",
      "word and doc :  is doc0\n",
      "word and doc :  is doc1\n",
      "word and doc :  is doc2\n",
      "word and doc :  an doc0\n",
      "word and doc :  an doc1\n",
      "word and doc :  an doc2\n",
      "word and doc :  exciting doc0\n",
      "word and doc :  exciting doc1\n",
      "word and doc :  exciting doc2\n",
      "word and doc :  field doc0\n",
      "word and doc :  field doc1\n",
      "word and doc :  field doc2\n",
      "word and doc :  that doc0\n",
      "word and doc :  that doc1\n",
      "word and doc :  that doc2\n",
      "word and doc :  enables doc0\n",
      "word and doc :  enables doc1\n",
      "word and doc :  enables doc2\n",
      "word and doc :  computers doc0\n",
      "word and doc :  computers doc1\n",
      "word and doc :  computers doc2\n",
      "word and doc :  to doc0\n",
      "word and doc :  to doc1\n",
      "word and doc :  to doc2\n",
      "word and doc :  learn doc0\n",
      "word and doc :  learn doc1\n",
      "word and doc :  learn doc2\n",
      "word and doc :  from doc0\n",
      "word and doc :  from doc1\n",
      "word and doc :  from doc2\n",
      "word and doc :  data doc0\n",
      "word and doc :  data doc1\n",
      "word and doc :  data doc2\n",
      "word and doc :  and doc0\n",
      "word and doc :  and doc1\n",
      "word and doc :  and doc2\n",
      "word and doc :  make doc0\n",
      "word and doc :  make doc1\n",
      "word and doc :  make doc2\n",
      "word and doc :  intelligent doc0\n",
      "word and doc :  intelligent doc1\n",
      "word and doc :  intelligent doc2\n",
      "word and doc :  decisions doc0\n",
      "word and doc :  decisions doc1\n",
      "word and doc :  decisions doc2\n",
      "word and doc :  deep doc0\n",
      "word and doc :  deep doc1\n",
      "word and doc :  deep doc2\n",
      "word and doc :  learning, doc0\n",
      "word and doc :  learning, doc1\n",
      "word and doc :  learning, doc2\n",
      "word and doc :  a doc0\n",
      "word and doc :  a doc1\n",
      "word and doc :  a doc2\n",
      "word and doc :  subset doc0\n",
      "word and doc :  subset doc1\n",
      "word and doc :  subset doc2\n",
      "word and doc :  of doc0\n",
      "word and doc :  of doc1\n",
      "word and doc :  of doc2\n",
      "word and doc :  uses doc0\n",
      "word and doc :  uses doc1\n",
      "word and doc :  uses doc2\n",
      "word and doc :  neural doc0\n",
      "word and doc :  neural doc1\n",
      "word and doc :  neural doc2\n",
      "word and doc :  networks doc0\n",
      "word and doc :  networks doc1\n",
      "word and doc :  networks doc2\n",
      "word and doc :  process doc0\n",
      "word and doc :  process doc1\n",
      "word and doc :  process doc2\n",
      "word and doc :  large doc0\n",
      "word and doc :  large doc1\n",
      "word and doc :  large doc2\n",
      "word and doc :  amounts doc0\n",
      "word and doc :  amounts doc1\n",
      "word and doc :  amounts doc2\n",
      "word and doc :  improve doc0\n",
      "word and doc :  improve doc1\n",
      "word and doc :  improve doc2\n",
      "word and doc :  accuracy doc0\n",
      "word and doc :  accuracy doc1\n",
      "word and doc :  accuracy doc2\n",
      "word and doc :  artificial doc0\n",
      "word and doc :  artificial doc1\n",
      "word and doc :  artificial doc2\n",
      "word and doc :  intelligence doc0\n",
      "word and doc :  intelligence doc1\n",
      "word and doc :  intelligence doc2\n",
      "word and doc :  covers doc0\n",
      "word and doc :  covers doc1\n",
      "word and doc :  covers doc2\n",
      "word and doc :  various doc0\n",
      "word and doc :  various doc1\n",
      "word and doc :  various doc2\n",
      "word and doc :  techniques, doc0\n",
      "word and doc :  techniques, doc1\n",
      "word and doc :  techniques, doc2\n",
      "word and doc :  including doc0\n",
      "word and doc :  including doc1\n",
      "word and doc :  including doc2\n",
      "word and doc :  mimic doc0\n",
      "word and doc :  mimic doc1\n",
      "word and doc :  mimic doc2\n",
      "word and doc :  human doc0\n",
      "word and doc :  human doc1\n",
      "word and doc :  human doc2\n",
      "word and doc :  solve doc0\n",
      "word and doc :  solve doc1\n",
      "word and doc :  solve doc2\n",
      "word and doc :  complex doc0\n",
      "word and doc :  complex doc1\n",
      "word and doc :  complex doc2\n",
      "word and doc :  problems doc0\n",
      "word and doc :  problems doc1\n",
      "word and doc :  problems doc2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': ['machine',\n",
       "  'learning',\n",
       "  'is',\n",
       "  'an',\n",
       "  'exciting',\n",
       "  'field',\n",
       "  'that',\n",
       "  'enables',\n",
       "  'computers',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'from',\n",
       "  'data',\n",
       "  'and',\n",
       "  'make',\n",
       "  'intelligent',\n",
       "  'decisions',\n",
       "  'deep',\n",
       "  'learning,',\n",
       "  'a',\n",
       "  'subset',\n",
       "  'of',\n",
       "  'uses',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'process',\n",
       "  'large',\n",
       "  'amounts',\n",
       "  'improve',\n",
       "  'accuracy',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'covers',\n",
       "  'various',\n",
       "  'techniques,',\n",
       "  'including',\n",
       "  'mimic',\n",
       "  'human',\n",
       "  'solve',\n",
       "  'complex',\n",
       "  'problems'],\n",
       " 'doc0': [0.0,\n",
       "  0.025341569256760274,\n",
       "  0.073117292116405,\n",
       "  0.07792428232550141,\n",
       "  0.0833773788062241,\n",
       "  0.08961123560704295,\n",
       "  0.09679939225560036,\n",
       "  0.10516889612989509,\n",
       "  0.11502179624219135,\n",
       "  0.0,\n",
       "  0.1433029527178584,\n",
       "  0.16133133152177487,\n",
       "  0.06790577308803089,\n",
       "  0.0,\n",
       "  0.2720076683837772,\n",
       "  0.331815854814202,\n",
       "  0.41571185526746884,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'doc1': [0.021340268847798126,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.02249916499324209,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.023789540477781174,\n",
       "  0.025234906080805482,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.02686468991711522,\n",
       "  0.057432422456130644,\n",
       "  0.09021908220808894,\n",
       "  0.09750379248389436,\n",
       "  0.21198738021882324,\n",
       "  0.12809009916813283,\n",
       "  0.14258504867347507,\n",
       "  0.16043882366186338,\n",
       "  0.18285868699536287,\n",
       "  0.21164425448424992,\n",
       "  0.24954349644445684,\n",
       "  0.3008222249467352,\n",
       "  0.3720510165922756,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'doc2': [0.021340268847798126,\n",
       "  0.02249916499324209,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.023789540477781174,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.050469812161610964,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.028719526208191713,\n",
       "  0.030841310685554287,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09021538806680054,\n",
       "  0.19499901934528385,\n",
       "  0.11609710592087708,\n",
       "  0.12805874634167255,\n",
       "  0.14254677941296776,\n",
       "  0.16039126836683387,\n",
       "  0.18279836120800072,\n",
       "  0.21156590281232002,\n",
       "  0.2494390159654593,\n",
       "  0.3006790040428499,\n",
       "  0.37184999196921437]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = ti.tf_idf()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4538e9-649c-41ec-8de6-5d4e23b5b836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>doc0</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021340</td>\n",
       "      <td>0.021340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>0.073117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>0.077924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exciting</td>\n",
       "      <td>0.083377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>field</td>\n",
       "      <td>0.089611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>that</td>\n",
       "      <td>0.096799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enables</td>\n",
       "      <td>0.105169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>computers</td>\n",
       "      <td>0.115022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022499</td>\n",
       "      <td>0.023790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>learn</td>\n",
       "      <td>0.143303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from</td>\n",
       "      <td>0.161331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data</td>\n",
       "      <td>0.067906</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025235</td>\n",
       "      <td>0.050470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>make</td>\n",
       "      <td>0.272008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>intelligent</td>\n",
       "      <td>0.331816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>decisions</td>\n",
       "      <td>0.415712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deep</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026865</td>\n",
       "      <td>0.028720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>learning,</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057432</td>\n",
       "      <td>0.030841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090219</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>subset</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097504</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>of</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211987</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uses</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>neural</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>networks</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>process</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>large</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211644</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>amounts</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>improve</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300822</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372051</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>artificial</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>intelligence</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>covers</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>various</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>techniques,</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>including</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mimic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>human</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>solve</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>complex</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>problems</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index      doc0      doc1      doc2\n",
       "0        machine  0.000000  0.021340  0.021340\n",
       "1       learning  0.025342  0.000000  0.022499\n",
       "2             is  0.073117  0.000000  0.000000\n",
       "3             an  0.077924  0.000000  0.000000\n",
       "4       exciting  0.083377  0.000000  0.000000\n",
       "5          field  0.089611  0.000000  0.000000\n",
       "6           that  0.096799  0.000000  0.000000\n",
       "7        enables  0.105169  0.000000  0.000000\n",
       "8      computers  0.115022  0.000000  0.000000\n",
       "9             to  0.000000  0.022499  0.023790\n",
       "10         learn  0.143303  0.000000  0.000000\n",
       "11          from  0.161331  0.000000  0.000000\n",
       "12          data  0.067906  0.023790  0.000000\n",
       "13           and  0.000000  0.025235  0.050470\n",
       "14          make  0.272008  0.000000  0.000000\n",
       "15   intelligent  0.331816  0.000000  0.000000\n",
       "16     decisions  0.415712  0.000000  0.000000\n",
       "17          deep  0.000000  0.026865  0.028720\n",
       "18     learning,  0.000000  0.057432  0.030841\n",
       "19             a  0.000000  0.090219  0.000000\n",
       "20        subset  0.000000  0.097504  0.000000\n",
       "21            of  0.000000  0.211987  0.000000\n",
       "22          uses  0.000000  0.128090  0.000000\n",
       "23        neural  0.000000  0.142585  0.000000\n",
       "24      networks  0.000000  0.160439  0.000000\n",
       "25       process  0.000000  0.182859  0.000000\n",
       "26         large  0.000000  0.211644  0.000000\n",
       "27       amounts  0.000000  0.249543  0.000000\n",
       "28       improve  0.000000  0.300822  0.000000\n",
       "29      accuracy  0.000000  0.372051  0.000000\n",
       "30    artificial  0.000000  0.000000  0.090215\n",
       "31  intelligence  0.000000  0.000000  0.194999\n",
       "32        covers  0.000000  0.000000  0.116097\n",
       "33       various  0.000000  0.000000  0.128059\n",
       "34   techniques,  0.000000  0.000000  0.142547\n",
       "35     including  0.000000  0.000000  0.160391\n",
       "36         mimic  0.000000  0.000000  0.182798\n",
       "37         human  0.000000  0.000000  0.211566\n",
       "38         solve  0.000000  0.000000  0.249439\n",
       "39       complex  0.000000  0.000000  0.300679\n",
       "40      problems  0.000000  0.000000  0.371850"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ti.to_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeb8b5-f3b5-4f39-ae8a-7df266b2b1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
