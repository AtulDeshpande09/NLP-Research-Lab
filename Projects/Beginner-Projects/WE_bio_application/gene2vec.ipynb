{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a45a552c-6354-48e6-a618-3b5532676336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Preprocess the data (tokenize into 6-mers and create context pairs)\n",
    "seqs = ['TCGTTG', 'CGTTGA', 'GTTGAG', 'TTGAGA', 'TGAGAT', 'GAGATG', 'AGATGG', 'GATGGG', 'ATGGGT', 'TGGGTC']\n",
    "window_size = 2  # context size for skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcd29573-cbfb-4d90-bbf8-d1cf762bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_pairs(seqs, window_size):\n",
    "    pairs = []\n",
    "    for seq in seqs:\n",
    "        for i, word in enumerate(seq):\n",
    "            # Define context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(seq), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if j != i:\n",
    "                    pairs.append((word, seq[j]))  # (target, context)\n",
    "    return pairs\n",
    "\n",
    "pairs = create_skipgram_pairs(seqs, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5bf9471-db29-4f12-8b5a-ca80c5014756",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([c for seq in seqs for c in seq]))  # Vocabulary of characters (6-mers)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}  # Map word to index\n",
    "idx2word = {i: word for word, i in word2idx.items()}  # Map index to word\n",
    "\n",
    "# 4. Create dataset of target and context pairs\n",
    "target_words = [word2idx[word[0]] for word in pairs]  # Target words (6-mers)\n",
    "context_words = [word2idx[word[1]] for word in pairs]  # Context words (characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043b116a-b0b9-4f9d-98c9-f2d536576921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, target):\n",
    "        target_emb = self.in_embedding(target)  # Embedding of target\n",
    "        context_emb = self.out_embedding.weight  # Context embeddings (output layer is shared)\n",
    "        score = torch.matmul(target_emb, context_emb.t())  # Dot product with context embeddings\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0727d0fc-ec08-4ee4-a739-9d4427acd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "model = SkipGramModel(len(vocab), embedding_dim)\n",
    "\n",
    "# 6. Set up optimizer and loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()  # This will be used to calculate loss over the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dad5ba8c-5cf6-4689-970e-52415eedb0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 285.3716446682811\n",
      "Epoch 2/100, Loss: 224.45380972325802\n",
      "Epoch 3/100, Loss: 211.65820042788982\n",
      "Epoch 4/100, Loss: 206.90768520534039\n",
      "Epoch 5/100, Loss: 205.70158591866493\n",
      "Epoch 6/100, Loss: 205.92478255927563\n",
      "Epoch 7/100, Loss: 206.45002116262913\n",
      "Epoch 8/100, Loss: 206.81446540355682\n",
      "Epoch 9/100, Loss: 206.9582861661911\n",
      "Epoch 10/100, Loss: 206.96845388412476\n",
      "Epoch 11/100, Loss: 206.92398042976856\n",
      "Epoch 12/100, Loss: 206.86333034932613\n",
      "Epoch 13/100, Loss: 206.80056509375572\n",
      "Epoch 14/100, Loss: 206.7402051538229\n",
      "Epoch 15/100, Loss: 206.683678150177\n",
      "Epoch 16/100, Loss: 206.6312806904316\n",
      "Epoch 17/100, Loss: 206.58279745280743\n",
      "Epoch 18/100, Loss: 206.53781494498253\n",
      "Epoch 19/100, Loss: 206.49590329825878\n",
      "Epoch 20/100, Loss: 206.45674999058247\n",
      "Epoch 21/100, Loss: 206.4201611727476\n",
      "Epoch 22/100, Loss: 206.38606256246567\n",
      "Epoch 23/100, Loss: 206.35443659126759\n",
      "Epoch 24/100, Loss: 206.32528126239777\n",
      "Epoch 25/100, Loss: 206.29859128594398\n",
      "Epoch 26/100, Loss: 206.2743180990219\n",
      "Epoch 27/100, Loss: 206.2523937523365\n",
      "Epoch 28/100, Loss: 206.23270520567894\n",
      "Epoch 29/100, Loss: 206.21512216329575\n",
      "Epoch 30/100, Loss: 206.19948816299438\n",
      "Epoch 31/100, Loss: 206.1856581568718\n",
      "Epoch 32/100, Loss: 206.17345401644707\n",
      "Epoch 33/100, Loss: 206.16272541880608\n",
      "Epoch 34/100, Loss: 206.1533108651638\n",
      "Epoch 35/100, Loss: 206.14506746828556\n",
      "Epoch 36/100, Loss: 206.13786789774895\n",
      "Epoch 37/100, Loss: 206.13158205151558\n",
      "Epoch 38/100, Loss: 206.12609827518463\n",
      "Epoch 39/100, Loss: 206.12132188677788\n",
      "Epoch 40/100, Loss: 206.1171581596136\n",
      "Epoch 41/100, Loss: 206.1135238558054\n",
      "Epoch 42/100, Loss: 206.1103596240282\n",
      "Epoch 43/100, Loss: 206.10759533941746\n",
      "Epoch 44/100, Loss: 206.10517747700214\n",
      "Epoch 45/100, Loss: 206.10306125879288\n",
      "Epoch 46/100, Loss: 206.1012113839388\n",
      "Epoch 47/100, Loss: 206.09958048164845\n",
      "Epoch 48/100, Loss: 206.09814921021461\n",
      "Epoch 49/100, Loss: 206.09687945246696\n",
      "Epoch 50/100, Loss: 206.09575629234314\n",
      "Epoch 51/100, Loss: 206.09475949406624\n",
      "Epoch 52/100, Loss: 206.09387066960335\n",
      "Epoch 53/100, Loss: 206.0930759459734\n",
      "Epoch 54/100, Loss: 206.09235882759094\n",
      "Epoch 55/100, Loss: 206.09171482920647\n",
      "Epoch 56/100, Loss: 206.09112358093262\n",
      "Epoch 57/100, Loss: 206.0905917584896\n",
      "Epoch 58/100, Loss: 206.09010258316994\n",
      "Epoch 59/100, Loss: 206.0896526426077\n",
      "Epoch 60/100, Loss: 206.08924095332623\n",
      "Epoch 61/100, Loss: 206.0888569355011\n",
      "Epoch 62/100, Loss: 206.08850115537643\n",
      "Epoch 63/100, Loss: 206.08816495537758\n",
      "Epoch 64/100, Loss: 206.08785419166088\n",
      "Epoch 65/100, Loss: 206.08755937218666\n",
      "Epoch 66/100, Loss: 206.08727796375751\n",
      "Epoch 67/100, Loss: 206.08701525628567\n",
      "Epoch 68/100, Loss: 206.08675886690617\n",
      "Epoch 69/100, Loss: 206.08651772141457\n",
      "Epoch 70/100, Loss: 206.0862848609686\n",
      "Epoch 71/100, Loss: 206.08606226742268\n",
      "Epoch 72/100, Loss: 206.08585350215435\n",
      "Epoch 73/100, Loss: 206.08564676344395\n",
      "Epoch 74/100, Loss: 206.08545072376728\n",
      "Epoch 75/100, Loss: 206.08525656163692\n",
      "Epoch 76/100, Loss: 206.08506946265697\n",
      "Epoch 77/100, Loss: 206.0848875194788\n",
      "Epoch 78/100, Loss: 206.0847134143114\n",
      "Epoch 79/100, Loss: 206.08453997969627\n",
      "Epoch 80/100, Loss: 206.08437460660934\n",
      "Epoch 81/100, Loss: 206.08421294391155\n",
      "Epoch 82/100, Loss: 206.0840531140566\n",
      "Epoch 83/100, Loss: 206.0838941037655\n",
      "Epoch 84/100, Loss: 206.08374167978764\n",
      "Epoch 85/100, Loss: 206.0835936665535\n",
      "Epoch 86/100, Loss: 206.0834462493658\n",
      "Epoch 87/100, Loss: 206.0833045244217\n",
      "Epoch 88/100, Loss: 206.0831632167101\n",
      "Epoch 89/100, Loss: 206.08302430808544\n",
      "Epoch 90/100, Loss: 206.0828871279955\n",
      "Epoch 91/100, Loss: 206.08275850117207\n",
      "Epoch 92/100, Loss: 206.08263143897057\n",
      "Epoch 93/100, Loss: 206.08250118792057\n",
      "Epoch 94/100, Loss: 206.08237592875957\n",
      "Epoch 95/100, Loss: 206.08225134015083\n",
      "Epoch 96/100, Loss: 206.08212988078594\n",
      "Epoch 97/100, Loss: 206.08200977742672\n",
      "Epoch 98/100, Loss: 206.081891477108\n",
      "Epoch 99/100, Loss: 206.08177453279495\n",
      "Epoch 100/100, Loss: 206.08166360855103\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for target, context in zip(target_words, context_words):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Convert target and context to tensors of appropriate shape\n",
    "        target_tensor = torch.tensor([target])  # Shape (1,)\n",
    "        context_tensor = torch.tensor([context])  # Shape (1,)\n",
    "        \n",
    "        # Forward pass\n",
    "        score = model(target_tensor)  # Get score (logits) for context words\n",
    "        loss = criterion(score, context_tensor)  # Loss against context word\n",
    "        loss.backward()  # Backpropagate\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a2cc4a5-540f-4636-84fb-5982d03da9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained Embeddings (for target words):\n",
      "T: tensor([-0.3007, -0.3713,  0.1541, -0.0021, -0.3340,  0.3385,  0.3076, -0.8643,\n",
      "         0.3777, -0.0364])\n",
      "G: tensor([-0.0101,  0.5077,  0.2132,  0.2711, -0.1591,  0.5186,  0.0081, -0.3631,\n",
      "        -0.1978,  0.1125])\n",
      "C: tensor([-0.1077,  1.0323,  0.2655,  0.5090, -0.6059,  0.7848, -0.0206, -0.7407,\n",
      "        -0.4208, -0.0199])\n",
      "A: tensor([-0.1957,  0.0563,  0.2357,  0.1602, -0.2924,  0.5408,  0.2069, -0.7741,\n",
      "         0.1306,  0.0580])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrained Embeddings (for target words):\")\n",
    "for i in range(len(vocab)):\n",
    "    print(f\"{idx2word[i]}: {model.in_embedding.weight.data[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac773ae5-81bd-4892-a24f-0e5e979c8707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
